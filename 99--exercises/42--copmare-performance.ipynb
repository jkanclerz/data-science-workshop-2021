{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jkanclerz/data-science-workshop-2021/blob/main/99--exercises/42--copmare-performance.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz -O spark-3.2.0-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.0-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/11 07:49:39 WARN Utils: Your hostname, Jakubs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.8.8 instead (on interface en0)\n",
      "21/12/11 07:49:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/jkanclerz/myplace/dydaktyka/przetwarzanie-dokumnetow/data-science-workshop-2021/40--spark/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/11 07:49:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/11 07:49:40 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Test it\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_by_age = spark.read.parquet(\"employees-by-age.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|wynagrodzenie|wiek|\n",
      "+-------------+----+\n",
      "|        12600|  65|\n",
      "|        13300|  65|\n",
      "|        13400|  65|\n",
      "|        12800|  65|\n",
      "|        12600|  65|\n",
      "|        13000|  65|\n",
      "|        13400|  65|\n",
      "|        12800|  65|\n",
      "|        12700|  65|\n",
      "|        13300|  65|\n",
      "|        13200|  65|\n",
      "|        12500|  65|\n",
      "|        13400|  65|\n",
      "|        13200|  65|\n",
      "|        12900|  65|\n",
      "|        12900|  65|\n",
      "|        12600|  65|\n",
      "|        13100|  65|\n",
      "|        13400|  65|\n",
      "|        13200|  65|\n",
      "+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_by_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf employee-by-age.json employee-by-sallary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_age.repartition(1)\\\n",
    "    .write\\\n",
    "    .partitionBy('wiek')\\\n",
    "    .format('json')\\\n",
    "    .save('employee-by-age.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_by_age.repartition(30)\\\n",
    "    .write\\\n",
    "    .partitionBy('wynagrodzenie')\\\n",
    "    .format('json')\\\n",
    "    .save('employee-by-sallary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"employee-by-age.json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|wiek|count|\n",
      "+----+-----+\n",
      "|  65|   72|\n",
      "|  53|   48|\n",
      "|  28|   55|\n",
      "|  26|   61|\n",
      "|  44|   48|\n",
      "|  47|   51|\n",
      "|  52|   54|\n",
      "|  40|   55|\n",
      "|  57|   48|\n",
      "|  54|   55|\n",
      "|  48|   50|\n",
      "|  64|   48|\n",
      "|  43|   46|\n",
      "|  37|   45|\n",
      "|  61|   47|\n",
      "|  35|   47|\n",
      "|  55|   44|\n",
      "|  39|   58|\n",
      "|  51|   42|\n",
      "|  50|   54|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 1.92 ms, sys: 1.21 ms, total: 3.13 ms\n",
      "Wall time: 398 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.groupby('wiek').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"employee-by-sallary.json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===========================================>            (37 + 1) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|wiek|count|\n",
      "+----+-----+\n",
      "|  29|   64|\n",
      "|  54|   55|\n",
      "|  34|   37|\n",
      "|  57|   48|\n",
      "|  32|   41|\n",
      "|  43|   46|\n",
      "|  31|   42|\n",
      "|  39|   58|\n",
      "|  51|   42|\n",
      "|  56|   60|\n",
      "|  52|   54|\n",
      "|  41|   43|\n",
      "|  33|   53|\n",
      "|  48|   50|\n",
      "|  44|   48|\n",
      "|  61|   47|\n",
      "|  62|   52|\n",
      "|  49|   41|\n",
      "|  35|   47|\n",
      "|  36|   50|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 9.47 ms, sys: 4.68 ms, total: 14.2 ms\n",
      "Wall time: 1.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:==================================================>     (43 + 1) / 48]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.groupby('wiek').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
